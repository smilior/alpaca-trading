# 仮説検証 リファレンス

## 仮説の立て方

### テンプレート

```
もし [条件/変更] ならば、
[測定可能な結果] になるはずだ。
それを検証するために [測定方法] を用いる。

成功基準: [定量的な基準]
失敗基準: [定量的な基準]
検証期間: [期間]
必要サンプル数: [取引回数]
```

### 良い仮説の例

```
仮説: 「VIX > 30 の日にエントリーしないフィルターを追加すれば、
       最大ドローダウンが30%以上改善するはずだ」

根拠: 過去のデータで、VIX > 30 の日にエントリーした取引の
      勝率は28%で、全体平均（52%）を大幅に下回っている。
      高VIX環境ではトレンドの方向性予測が困難であり、
      スプレッドも拡大するためスリッページが大きくなる。

測定方法: VIXフィルターなしとありの並行運用（ペーパー）
成功基準: MDD 30%改善、かつシャープレシオの低下が0.1未満
失敗基準: シャープレシオが0.2以上低下
検証期間: 4週間（約20営業日）
必要サンプル数: 各条件で最低20取引
```

### 悪い仮説の例

```
❌ 「パラメータを最適化すれば成績が上がる」
→ 具体的な仮説がない。何を変えるか、なぜ変えるかが不明。

❌ 「AIを使えばもっと儲かる」
→ 漠然としすぎ。具体的にAIの何をどう使うかが不明。

❌ 「ストップを広げれば勝率が上がる」
→ 勝率は上がるが期待値が下がる可能性がある。
  トレードオフを考慮していない。
```

## A/Bテストの設計

### トレーディングにおけるA/Bテスト

通常のA/Bテスト（ランダム割り当て）はトレーディングでは難しい。代替手法を使う。

### 方法1: 時系列分割（Before/After）

```
Before期間（変更前）: 4週間
→ KPI計測

変更適用

After期間（変更後）: 4週間
→ KPI計測

比較: Before vs After
```

**注意点**: 市場環境が変わっている可能性がある。VIX等で市場環境を補正する。

### 方法2: 並行ペーパートレーディング

```
戦略A（現行）: ペーパーアカウント1で実行
戦略B（変更版）: ペーパーアカウント2で実行

同じ期間に同じ市場環境で並行実行
→ 市場環境の差異が排除される

4週間後にKPIを比較
```

**Alpacaではペーパーアカウントは1つ**のため、片方をバックテストで代替するか、シミュレーションで実施する。

### 方法3: 銘柄分割

```
ユニバースの銘柄を2グループに分ける
（ランダムまたはセクターバランスを考慮）

グループA: 現行戦略で取引
グループB: 変更版戦略で取引

4週間後にグループ別KPIを比較
```

## 統計的に有意な結果の判断

### サンプルサイズの目安

```
効果の大きさ（改善幅）に応じた必要サンプル数:

大きな改善（シャープレシオ0.5以上改善）: 30取引
中程度の改善（シャープレシオ0.2-0.5改善）: 100取引
小さな改善（シャープレシオ0.2未満改善）: 300取引

一般的な推奨: 最低100取引
```

### p値の計算

```python
from scipy import stats
import numpy as np

def test_strategy_improvement(returns_before, returns_after):
    """戦略変更前後のリターンの差が有意かテスト"""

    # ウェルチのt検定（等分散を仮定しない）
    t_stat, p_value = stats.ttest_ind(
        returns_after, returns_before,
        equal_var=False
    )

    # 効果量（Cohen's d）
    pooled_std = np.sqrt(
        (returns_before.std()**2 + returns_after.std()**2) / 2
    )
    cohens_d = (returns_after.mean() - returns_before.mean()) / pooled_std

    return {
        'mean_before': returns_before.mean(),
        'mean_after': returns_after.mean(),
        'improvement': returns_after.mean() - returns_before.mean(),
        't_statistic': t_stat,
        'p_value': p_value,
        'significant': p_value < 0.05,
        'cohens_d': cohens_d,
        'effect_size': 'large' if abs(cohens_d) > 0.8
                      else 'medium' if abs(cohens_d) > 0.5
                      else 'small'
    }
```

### 判断基準

| p値 | 解釈 | アクション |
|-----|------|-----------|
| < 0.01 | 非常に有意 | 変更を採用 |
| 0.01 - 0.05 | 有意 | 変更を採用（ただし注視） |
| 0.05 - 0.10 | やや有意 | 追加データで再検証 |
| > 0.10 | 有意でない | 変更を採用しない |

## 多重比較の問題

```
⚠️ 警告: 多くの変更をテストすると、偶然有意になるものが出る

例: 20個のパラメータ変更をテスト
→ p < 0.05 が偶然1つ出る確率: 1 - 0.95^20 = 64%

対策:
1. Bonferroni補正: p値の基準を（0.05 / テスト数）に引き下げ
   20個テストなら p < 0.0025 を基準にする
2. そもそもテスト数を少なく保つ（仮説に基づく変更のみ行う）
```

## 改善サイクルの記録テンプレート

```markdown
## 改善サイクル記録 #{cycle_number}

### 日付
開始: {start_date} → 終了: {end_date}

### 仮説
{hypothesis}

### 変更内容
- 変更前: {before}
- 変更後: {after}

### 結果
| KPI | 変更前 | 変更後 | 差分 | p値 |
|-----|--------|--------|------|-----|
| シャープレシオ | | | | |
| 勝率 | | | | |
| PF | | | | |
| MDD | | | | |

### 判断
[ ] 採用 / [ ] 不採用 / [ ] 追加検証

### 学んだこと
{lessons}

### 次のアクション
{next_steps}
```
